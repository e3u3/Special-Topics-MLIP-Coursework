{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  A53299801 - SIDDARTH MEENAKSHI SUNDARAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import MNISTtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1. Shape of xtrain is  (784, 60000)\n",
      "          Shape of ltrain is  (60000,)\n",
      "          Size of training dataset is  60000\n",
      "          Feature dimension is  784\n"
     ]
    }
   ],
   "source": [
    "#1 Loading dataset and checking shape of the training dataset\n",
    "\n",
    "#help(MNISTtools.load)\n",
    "#help(MNISTtools.show)\n",
    "xtrain, ltrain = MNISTtools.load(dataset=\"training\", path=None) \n",
    "print(\"Answer 1. Shape of xtrain is \", np.shape(xtrain))\n",
    "print(\"          Shape of ltrain is \", np.shape(ltrain))\n",
    "print(\"          Size of training dataset is \",xtrain.shape[1])\n",
    "print(\"          Feature dimension is \",xtrain.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2. Displaying image at index 42:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Label of image is  7\n"
     ]
    }
   ],
   "source": [
    "#2 Displaying Image and corresponding label\n",
    "\n",
    "print(\"Answer 2. Displaying image at index 42:\")\n",
    "MNISTtools.show(xtrain[:, 42])\n",
    "print(\"          Label of image is \", ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q3} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 3. xtrain range is [ 0 ,  255 ]\n",
      "          xtrain type is  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#3 Finding xtrain range and type\n",
    "\n",
    "#xtrain_max = np.max(xtrain)\n",
    "#xtrain_min = np.min(xtrain)\n",
    "print(\"Answer 3. xtrain range is [\", np.min(xtrain), \", \", np.max(xtrain), \"]\")\n",
    "print(\"          xtrain type is \",type(xtrain))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q4} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Normalizing and Updating xtrain\n",
    "\n",
    "xtrain = xtrain.astype(np.float32)\n",
    "def normalize_MNIST_images(x):\n",
    "    x = -1 + (2*x/255)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 4. Min of normalized xtrain -1.0\n",
      "          Max of normalized xtrain 1.0\n",
      "          Range of normalized xtrain is [ -1.0 ,  1.0 ]\n"
     ]
    }
   ],
   "source": [
    "#xtrain = (np.interp(xtrain, (np.min(xtrain), np.max(xtrain)), (-1, +1)))\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print(\"Answer 4. Min of normalized xtrain\", np.min(xtrain))\n",
    "print(\"          Max of normalized xtrain\", np.max(xtrain))\n",
    "print(\"          Range of normalized xtrain is [\", np.min(xtrain), \", \", np.max(xtrain), \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q5} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Shape of dtrain as  (10, 60000)\n",
      "Answer 5. One hot code for index 42 is  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "          Label for index 42 is  7\n"
     ]
    }
   ],
   "source": [
    "#5 Converting label to one hot code.\n",
    "\n",
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    for i in range(lbl.max()):\n",
    "        d[lbl, np.arange(lbl.size)] = 1\n",
    "    return d\n",
    "dtrain = label2onehot(ltrain)\n",
    "print(\"Checking Shape of dtrain as \", np.shape(dtrain))\n",
    "print(\"Answer 5. One hot code for index 42 is \", dtrain[:,42])\n",
    "print(\"          Label for index 42 is \", ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : We can see that One hot code of label at index 42 corresponds to value of label at index 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q6} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing ltrain to onehot2label(dtrain)\n",
      "Answer 6. True\n"
     ]
    }
   ],
   "source": [
    "#6 Converting one hot code to label\n",
    "\n",
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0)\n",
    "    return lbl\n",
    "\n",
    "print(\"Comparing ltrain to onehot2label(dtrain)\")\n",
    "print(\"Answer 6.\", set(onehot2label(dtrain))==set(ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : In Set comparison, True means arrays are same \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q7} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 \n",
    "#Defining softmax function - An activation function\n",
    "def softmax(a):\n",
    "    M = a.max(axis=0)\n",
    "    y = np.exp(a-M)/(np.exp(a-M).sum(axis=0))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 7. Verifying Softmax fn.\n",
      "          Sum of probabilities at some location: 1.0 \n",
      "          1.0 implies no numerical loss\n"
     ]
    }
   ],
   "source": [
    "y = softmax(xtrain)\n",
    "print(\"Answer 7. Verifying Softmax fn.\")\n",
    "#print(np.shape(y[59]))\n",
    "#print(np.shape(y))\n",
    "print(\"          Sum of probabilities at some location:\",np.sum(y[:,59]), \"\\n          1.0 implies no numerical loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q8, Q9, Q10 : PROOFS solved by hand} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q10} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 Defining softmaxp fn. - Also an activation fn. that is the derivative of softmax fn.\n",
    "\n",
    "def softmaxp(a, e):\n",
    "    y = softmax(a)\n",
    "    d = np.multiply(y, e) - ((np.multiply(y, e)).sum(axis=0))*(y) #Derivative of softmax()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q11} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 11. Checking softmaxp () and its implementation by numerical approximations\n",
      "\t   4.869308829892089e-07 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "#11 Checking softmaxp fn. and its implementation by numerical approximations\n",
    "\n",
    "eps = 1e-6  # finite difference step\n",
    "a = np.random.randn(10, 200)# random inputs\n",
    "e = np.random.randn(10, 200)# random directions\n",
    "\n",
    "diff = softmaxp(a, e)\n",
    "diff_approx = (softmax(a + eps*e) - softmax(a)) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(\"Answer 11. Checking softmaxp () and its implementation by numerical approximations\")\n",
    "print(\"\\t  \",rel_error,'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As error found is smaller than 1e-6, the softmaxp() works as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q12} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 Defining relu() and its directional derivative - called as relup()\n",
    "#   Both are activation functions - REctified Linear Units(RELU)\n",
    "\n",
    "def relu(a):\n",
    "    return np.maximum(a, 0)       # RELU fn. gives a for a>0 and 0 for a<=0\n",
    "\n",
    "def relup(a,e):\n",
    "    c = np.maximum(a, 0) \n",
    "    c[c>0] = 1\n",
    "    c = np.multiply(c,e)          # RELUp fn. gives e for a>0 and 0 for a<=0\n",
    "    return c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 12. Checking relup () and its implementation by numerical approximations\n",
      "\t   3.8815552900261954e-11 should be smaller than 1e-6\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6  # finite difference step\n",
    "a = np.random.randn(10, 200)# random inputs\n",
    "e = np.random.randn(10, 200)# random directions\n",
    "\n",
    "diff = relup(a, e)\n",
    "diff_approx = (relu(a + eps*e) - relu(a)) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(\"Answer 12. Checking relup () and its implementation by numerical approximations\")\n",
    "print(\"\\t  \",rel_error,'should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As error found is smaller than 1e-6, the relup() works as intended\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q13} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "#13 Initializing Shallow Networks\n",
    "\n",
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.)\n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.))\n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.))\n",
    "    return W1, b1, W2, b2\n",
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shallow network is initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q14} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14 Defining Forward propagation for the shallow network\n",
    "\n",
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "yinit = forwardprop_shallow(xtrain, netinit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is defined and yinit is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q15} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 15.  0.26618090376248654 should be around .26\n"
     ]
    }
   ],
   "source": [
    "#15 Defining the evaluation loss using average of cross entropy using predictions(y) and desired one-hot code(d)\n",
    "\n",
    "def eval_loss(y, d):\n",
    "    \n",
    "    e = -((np.multiply(d,np.log(y))))\n",
    "    e = np.mean(e)\n",
    "    return e\n",
    "\n",
    "print(\"Answer 15. \",eval_loss(yinit, dtrain), 'should be around .26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eval_loss has been defined and the loss is close to the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q16} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 16. Percentage of initial misclassified samples 84.53833333333334 %\n"
     ]
    }
   ],
   "source": [
    "#16 Evaluating the performance of the network\n",
    "\n",
    "def eval_perfs(y, lbl):\n",
    "    y = onehot2label(y)\n",
    "    n = 0\n",
    "    n = np.equal(y,lbl)\n",
    "    p = sum(n)                                #Number of correctly labelled sample is the number of true values in n\n",
    "   \n",
    "    performance = ((y.shape[0]-p)/y.shape[0]) #Percentage of misclassified samples is \n",
    "    performance = 100*performance             #Number of wrongly labelled samples / total no of samples \n",
    "    return performance\n",
    "\n",
    "print(\"Answer 16. Percentage of initial misclassified samples\" , eval_perfs(yinit, ltrain), \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: The initial misclassified samples are about 89% which means that about 53573 samples are wrongly classified initially. Only about 6427 samples are classified correcltly. Hence, without any training, this network can classify correctly less than 11% of the time. The network is not suitable for classification without training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q17} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "#Defining update_shallow to complete one backpropagation.\n",
    "\n",
    "def update_shallow(x, d, net, gamma=.05):\n",
    "      \n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "\n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    #Forward propagation\n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "        \n",
    "    delta2 = softmaxp(a2, -d/y) # -d/y is the error to be backpropagated.\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    # Weights and biases are updated\n",
    "    \n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis = 1, keepdims = True)\n",
    "    b1 = b1 - gamma * delta1.sum(axis = 1, keepdims = True)\n",
    "   \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single backpropagation update for shallow network is obtained using the above function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### See at end for error derivation proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q18} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer 18. Loss and Percentage Training errors after backpropagation\n",
      "            Loss after T = 0  is  0.22845196019115085\n",
      "            Perf after T = 0  is  85.00666666666666\n",
      "            Loss after T = 1  is  0.2144997854733571\n",
      "            Perf after T = 1  is  74.21666666666667\n",
      "            Loss after T = 2  is  0.2056885867904884\n",
      "            Perf after T = 2  is  70.76166666666667\n",
      "            Loss after T = 3  is  0.19920451451603247\n",
      "            Perf after T = 3  is  66.47333333333333\n",
      "            Loss after T = 4  is  0.19081469483954824\n",
      "            Perf after T = 4  is  62.385000000000005\n",
      "            Loss after T = 5  is  0.1868210103617379\n",
      "            Perf after T = 5  is  60.36833333333333\n",
      "            Loss after T = 6  is  0.1773707040696068\n",
      "            Perf after T = 6  is  54.785\n",
      "            Loss after T = 7  is  0.17365475928678384\n",
      "            Perf after T = 7  is  54.67166666666666\n",
      "            Loss after T = 8  is  0.16483368042744984\n",
      "            Perf after T = 8  is  48.88333333333333\n",
      "            Loss after T = 9  is  0.1616266484364539\n",
      "            Perf after T = 9  is  49.535000000000004\n",
      "            Loss after T = 10  is  0.1535214180202826\n",
      "            Perf after T = 10  is  44.45666666666666\n",
      "            Loss after T = 11  is  0.15147846902238005\n",
      "            Perf after T = 11  is  45.788333333333334\n",
      "            Loss after T = 12  is  0.14414978569240197\n",
      "            Perf after T = 12  is  41.68666666666667\n",
      "            Loss after T = 13  is  0.1441225336269138\n",
      "            Perf after T = 13  is  44.06\n",
      "            Loss after T = 14  is  0.13684870678316144\n",
      "            Perf after T = 14  is  39.973333333333336\n",
      "            Loss after T = 15  is  0.13852692105265907\n",
      "            Perf after T = 15  is  43.475\n",
      "            Loss after T = 16  is  0.13111923279970408\n",
      "            Perf after T = 16  is  38.61333333333333\n",
      "            Loss after T = 17  is  0.13291654842282893\n",
      "            Perf after T = 17  is  42.16666666666667\n",
      "            Loss after T = 18  is  0.12350280670266525\n",
      "            Perf after T = 18  is  35.47\n",
      "            Loss after T = 19  is  0.12380961348612261\n",
      "            Perf after T = 19  is  38.565\n"
     ]
    }
   ],
   "source": [
    "#18 Updating Shallow network using Backpropagation.\n",
    "\n",
    "def backprop_shallow(x, d, net, T, gamma=.05):\n",
    "    y = forwardprop_shallow(xtrain, netinit) \n",
    "    #print(eval_loss(y,d))\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        net=update_shallow(x,d,net)     # Updating the net for T iterations by changing Weights and Biases using BackProp\n",
    "        y = forwardprop_shallow(x,net)  # Computing the Forward propagation after updation\n",
    "        print(\"            Loss after T =\",t,\" is \", eval_loss(y,d))   # Loss after each iteration\n",
    "        print(\"            Perf after T =\",t,\" is \", eval_perfs(y,lbl))# Performance after each iteratioon\n",
    "    return net\n",
    "\n",
    "print(\" Answer 18. Loss and Percentage Training errors after backpropagation\")\n",
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the error reduces steadily more or less from 81% at T=0 to 34% error after T=19, there are some exceptions in the variation, but generally as the iteration increases, we can see that error decreases along with the loss which reduces from 0.22 to 0.11. Hence, we are getting a better trained network as T increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q19} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Answer 19. Size of Testing set is \n",
      "           xtest shape is (784, 10000)\n",
      "           ltest shape is (10000,)\n",
      "           Testing Performance of our network on testing dataset\n",
      "           Testing Loss 0.12189442285569678\n",
      "           Testing Perf 38.279999999999994\n"
     ]
    }
   ],
   "source": [
    "#19 LOADING TESTING SETS\n",
    "xtest, ltest = MNISTtools.load(dataset=\"testing\", path=None) #Loading testing sets into xtest and ltest\n",
    "xtest = xtest.astype(np.float32)\n",
    "xtest = normalize_MNIST_images(xtest)     #Normalizing xtest values from [0, 255] to [-1, 1] values\n",
    "print(np.max(xtest))\n",
    "dtest = label2onehot(ltest)\n",
    "print(\"Answer 19. Size of Testing set is\",\"\\n           xtest shape is\", xtest.shape)\n",
    "print(\"           ltest shape is\", ltest.shape)\n",
    "#Testing Performance of network on testing dataset\n",
    "y = forwardprop_shallow(xtest, nettrain)\n",
    "print(\"           Testing Performance of our network on testing dataset\")\n",
    "print(\"           Testing Loss\", eval_loss(y,dtest)) \n",
    "print(\"           Testing Perf\", eval_perfs(y,ltest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained network which reaches an error of 34.35% in our training set produces an error of 46.07% in testing set. Also the training loss is 0.11 and the testing loss is 0.159."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q20} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 20. Testing performance on training dataset\n",
      "EPOCH ->  0\n",
      "Training Loss 0.028983610025401285\n",
      "Training Perf 8.478333333333333\n",
      "EPOCH ->  1\n",
      "Training Loss 0.022536804752878413\n",
      "Training Perf 6.806666666666667\n",
      "EPOCH ->  2\n",
      "Training Loss 0.018420219681529286\n",
      "Training Perf 5.246666666666667\n",
      "EPOCH ->  3\n",
      "Training Loss 0.01708914685654354\n",
      "Training Perf 4.921666666666667\n",
      "EPOCH ->  4\n",
      "Training Loss 0.013908752005279223\n",
      "Training Perf 4.04\n"
     ]
    }
   ],
   "source": [
    "#20 Running backpropagation as minibatches for 5 epochs with 100 minibatches\n",
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05):\n",
    "    N = x.shape[1]\n",
    "    NB = int((N+B-1)/B)\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(T):\n",
    "        shuffled_indices = np.random.permutation(range(N))\n",
    "        for l in range(NB):\n",
    "            minibatch_indices = shuffled_indices[B*l:min(B*(l+1), N)]\n",
    "            net = update_shallow(x[:, minibatch_indices], d[:, minibatch_indices], net, gamma) #Using interger array indexing\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print(\"EPOCH -> \", t)\n",
    "        print(\"Training Loss\", eval_loss(y,d))        #Evaluating loss of network after epoch\n",
    "        print(\"Training Perf\", eval_perfs(y,lbl))     #Evaluating Performance of network after epoch\n",
    "    return net\n",
    "\n",
    "print(\"Answer 20. Testing performance on training dataset\")\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\textbf {Q21} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 21. Performance of the final network on testing dataset\n",
      "           Testing Loss 0.014377454602171874\n",
      "           Testing Perf 4.3\n"
     ]
    }
   ],
   "source": [
    "#21 \n",
    "y = forwardprop_shallow(xtest, netminibatch)\n",
    "print(\"Answer 21. Performance of the final network on testing dataset\")\n",
    "print(\"           Testing Loss\", eval_loss(y,dtest)) \n",
    "print(\"           Testing Perf\", eval_perfs(y,ltest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training error is about 4.34% and the testing error is about 8.93%. Hence, our network can classify the numbers with an accuracy of 91.07% in the testing dataset. \n",
    "\n",
    "Inference:\n",
    "We see that the testing error obtained by using minibatches is much lesser than the error obtained from backpropagation in lesser number of epochs and the computation is faster.\n",
    "\n",
    "RESULT:\n",
    "Training accuracy = 95.66% ; Testing accuracy = 91.07%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
